# Creating standalone executable bundles

This document provides a short description about producing ahead-of-time
compiled executable bundles. The motivation for this work is to remove the cost
of compile time by allowing the users of Glow to compile the package ahead of
time.

## Overview

A bundle is a self-contained compiled network model that can be used to execute
the model in a standalone mode.

## Producing a bundle

It is possible to use the Glow library to produce bundles. On the CPU, the
bundles are object files that can be linked with some executable. On other
architectures the bundle may look completly different.

This document demonstrates how to produce a bundle for the host CPU using the
'loader' tool.  We use the flag `-emit-bundle` to specify the output directory.

```
loader image_file -image_mode=0to1 -d network_model_name -jit -emit-bundle output_directory_name
```

The command above would compile the neural network model from the
`network_model_directory_name` and generate a bundle consisting of two files in
the directory `output_directory_name`.

The first file is named `network_model_name.o` and contains the compiled code of
the network model. It is a regular object file that can be linked other files in
your project.  The second file is named `network_model_name.weights` and
contains the weights required to run the compiled model.

## APIs exposed by bundles

This section describes the APIs that the CPU bundle exposes. Other targets may
expose a completly different API.

Each bundle exposes two symbols named `network_model_name` and
`network_model_name_config`.  The `network_model_name` is the name of the
auto-generated function that implements the network model. This symbol always
has the following signature:

```c++
extern "C" void network_model_name(uint8_t *constantWeightVars,
                                   uint8_t *mutableWeightVars,
                                   uint8_t *activations);
```
The parameters of this function are the base addresses of the memory areas for
constant weights variables, mutable weights variables (i.e. inputs and outputs)
and activations.

The `network_model_name_config` is a symbol that contains the configuration of
the compiled network. The type of this symbol is always the following struct:
```c++
struct BundleConfig {
  // Size of the constant weight variables memory area.
  size_t constantWeightVarsMemSize;
  // Size of the mutable weight variables memory area.
  size_t mutableWeightVarsMemSize;
  // Size of the activations memory area.
  size_t activationsMemSize;
  // Alignment to be used for weights and activations.
  size_t alignment;
  // Number of symbols in the symbol table.
  size_t numSymbols;
  // Symbol table.
  const SymbolTableEntry *symbolTable;
};
```
This configuration is supposed to be used by the client code to allocate the
required amounts of memory for each of the memory areas, before invoking the
`network_model_name` function to run the network.

Clients also use `BundleConfig` to perform the symbol table lookups when they
need to find information about an input or output variable.
The SymbolTableEntry has always the following structure:
```c++
struct SymbolTableEntry {
  // Name of a variable.
  const char *name;
  // Offset of the variable inside the memory area.
  size_t offset;
  // The number of elements inside this variable.
  size_t size;
  // The kind of the variable. 1 if it is a mutable variable, 0 otherwise.
  char kind;
};
```

Offsets of constants are offsets inside the memory area for constant weights.
Offset of mutable variables are offsets inside the memory area for mutable
weights.

## How to use the bundle

This section describes the use of the CPU bundle. Other targets may have
different interfaces.

To integrate the artifacts generated by the loader into your project, you
generally need to do the following:
* You need to link with the generated object file `network_model_name.o`.
* You need to allocate the memory for constant weights variables,
mutable weights variables (i.e. inputs and outputs) and activations based on the
memory area sizes provided by `network_model_name_config`.
* You need to load the content of the auto-generated `network_model_name.weights`
file into the constant weights variables memory area.
* And need to initialize the mutable weights area with inputs (e.g. image data)
* And finally, you need to invoke the `network_model_name` function with 3
parameters that are base addresses of the the memory areas for constant weights variables,
mutable weights variables and activations.
* After `network_model_name` has returned, you can find the results of the mutable weights
variables area.


## A step-by-step example for the lenet_mnist network model

There is a concrete example of integrating a network model with a project.  You
can find it in the `examples/compile_lenet_mnist` directory in the Glow
repository.

To build and run this example, you just need to run the
`build_lenet_mnist_standalone.sh` script in the `examples/compile_lenet_mnist`
directory. You may need to adjust the environment variables at the top to match
your setup.

The script performs the following steps:
* It downloads the lenet_mnist network model in the Caffe2 format.
* It generates the bundle files using the Glow loader as described above.
  The concrete command line looks like this:
  `loader tests/images/mnist/5_1087.png -image_mode=0to1 -d lenet_mnist -jit -emit-bundle build`
  It reads the network model from `lenet_mnist` and generates the `lenet_mnist.o`
  and `lenet_mnist.weights` files into the `build` directory.
* Then it compiles the `lenet_mnist_standalone.cpp` file, which is the main file of the project.
  This source file gives a good idea about how to interface with an auto-generated bundle.
  It contains the code for interfacing with with the auto-generated bundle.
  *  It allocated the memory areas based on their memory sizes provided in `lenet_mnist_config`.
  *  Then it loads the weights from the auto-generated `lenet_mnist.weights` file.
  *  It loads the input image, pre-processes it and puts it into the mutable weight variables
     memory area.
  *  Once evertything is setup, it invokes the compiled network model by calling the
     `lenet_mnist` function from the `lenet_mnist.o` object file.
* Then it links the user-defined `lenet_mnist_standalone.o` and auto-generated `lenet_mnist.o`
  into a standalone executable file called `lenet_mnist_standalone`
* Finally, it runs this standalone executable with mnist images as inputs and outputs the
results of the network model execution.

