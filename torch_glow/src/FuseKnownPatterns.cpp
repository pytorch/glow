/**
 * Copyright (c) Glow Contributors. See CONTRIBUTORS file.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include "FuseKnownPatterns.h"

#include <glog/logging.h>

#include <torch/csrc/jit/custom_operator.h>
#include <torch/csrc/jit/passes/alias_analysis.h>
#include <torch/csrc/jit/passes/common_subexpression_elimination.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
#include <torch/csrc/jit/passes/subgraph_rewrite.h>
#include <torch/csrc/jit/passes/utils/subgraph_utils.h>

namespace glow {
namespace {
/// This pass fuse the quantized::conv_prepack + quantized::conv2d generated by
/// JIT back to quantized::unpacked_conv2d since we dont have
/// quantized::conv_prepack in glow. However regular packed conv's
/// implementation in glow is still needed.
void fuseConvPrepack(std::shared_ptr<torch::jit::Graph> &graph) {
  std::string convPrepackPattern = R"IR(
graph(%input, %w, %b, %4, %5, %6, %7, %8, %9, %10, %11, %12):
  %prepacked_weight = quantized::conv_prepack(%w, %b, %4, %5, %6, %7)
  %res = quantized::conv2d(%input, %prepacked_weight, %8, %9, %10, %7, %11, %12)
  return (%res))IR";

  std::string convFused = R"IR(
graph(%input, %w, %b, %4, %5, %6, %7, %8, %9, %10, %11, %12):
  %res = glow::unpacked_quantized_conv2d(%input, %w, %b, %8, %9, %10, %7, %11, %12)
  return (%res))IR";

  // Replace conv_prepack + conv2d to unpacked_quantized_conv2d
  torch::jit::SubgraphRewriter convToUnpackedConv;
  convToUnpackedConv.RegisterRewritePattern(convPrepackPattern, convFused);
  convToUnpackedConv.runOnGraph(graph);
}

void fuseLinearPrepack(std::shared_ptr<torch::jit::Graph> &graph) {
  std::string beforePattern = R"IR(
graph(%input, %weights, %bias, %scale, %zero_point):
  %packed_params = quantized::linear_prepack(%weights, %bias)
  %res = quantized::linear(%input, %packed_params, %scale, %zero_point)
  return (%res))IR";

  std::string afterPattern = R"IR(
graph(%input, %weights, %bias, %scale, %zero_point):
  %res = glow::unpacked_quantized_linear(%input, %weights, %bias, %scale, %zero_point)
  return (%res))IR";

  // Replace linear_prepack + quantized::linear to
  // glow::unpacked_quantized_linear
  torch::jit::SubgraphRewriter rewriter;
  rewriter.RegisterRewritePattern(beforePattern, afterPattern);
  rewriter.runOnGraph(graph);
}

void fuseNumToTensorToNum(std::shared_ptr<torch::jit::Graph> &graph) {
  std::string originalPat = R"IR(
graph(%input):
  %res1 = prim::NumToTensor(%input)
  %res2 = aten::Int(%res1)
  return (%res2))IR";

  std::string replacementPat = R"IR(
graph(%input):
  return (%input))IR";

  torch::jit::SubgraphRewriter rewriter;
  rewriter.RegisterRewritePattern(originalPat, replacementPat);
  rewriter.runOnGraph(graph);
}

/// Registers an operator with symbol \p opName but with no implementation.
/// Dummy operators can be used by glow-specific fusion passes prior to loading
/// a glow graph in order to eliminate intermediate values that are unnecessary
/// to Glow such as those created by quantization packing nodes.
void registerDummyOperator(const char *opName) {
  auto options = c10::OperatorOptions();
  options.setAliasAnalysis(at::AliasAnalysisKind::PURE_FUNCTION);

  torch::jit::RegisterOperators op({torch::jit::Operator(
      at::Symbol::fromQualString(opName),
      [](const torch::jit::Node *node) -> torch::jit::Operation {
        LOG(FATAL) << "Operator \"" << (*node)
                   << "\" has no implementation and is meant only as a "
                      "placeholder while fusing ops to run with Glow";
      },
      options)});
}

/// To judge if we can fuse the current node into a prim::FusedConcat.
bool isFusableConcatNode(torch::jit::Node *node) {
  if (node->kind() != at::aten::cat ||
      !node->is_constant(torch::jit::attr::dim)) {
    return false;
  }

  auto inputNode = node->namedInput(torch::jit::attr::tensors)->node();
  if (inputNode->kind() != at::prim::ListConstruct) {
    return false;
  }
  if (inputNode->output()->uses().size() > 1) {
    return false;
  }

  return true;
}

/// Add one prim::FusedConcat before current node.
/// The current node and the list construct node before it will become dead
/// node.
torch::jit::Node *createFusedConcat(torch::jit::Node *node) {
  AT_ASSERT(node->kind() == at::aten::cat);
  torch::jit::Graph *graph = node->owningGraph();
  torch::jit::Node *inputNode =
      node->namedInput(torch::jit::attr::tensors)->node();
  int64_t dim = node->get<int64_t>(torch::jit::attr::dim).value();

  torch::jit::Node *fusedConcatNode =
      graph->create(at::prim::FusedConcat, inputNode->inputs())
          ->i_(torch::jit::attr::dim, dim);
  fusedConcatNode->insertBefore(inputNode);
  fusedConcatNode->output()->copyMetadata(node->output());
  node->output()->replaceAllUsesWith(fusedConcatNode->output());

  return inputNode;
}

/// Fuse PyTorch ListConstruct and Concat node intp prim::FusedConcat node
void fuseConcat(std::shared_ptr<torch::jit::Graph> &graph) {
  auto block = graph->block();
  for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
    if (isFusableConcatNode(*it)) {
      // Here we relay on EliminateDeadCode to remove useless node in graph
      // and we dont remove them directly
      createFusedConcat(*it);
    }
  }
}
} // namespace

void fuseKnownPatterns(std::shared_ptr<torch::jit::Graph> &graph) {
  // Register dummy nodes used by custom fusers.
  static std::once_flag onceFlag;
  std::call_once(onceFlag, []() {
    registerDummyOperator("glow::unpacked_quantized_linear");
    registerDummyOperator("glow::unpacked_quantized_conv2d");
  });

  fuseConcat(graph);
  fuseConvPrepack(graph);
  fuseLinearPrepack(graph);
  fuseNumToTensorToNum(graph);
  EliminateCommonSubexpression(graph);
  EliminateDeadCode(graph);
}
} // namespace glow
